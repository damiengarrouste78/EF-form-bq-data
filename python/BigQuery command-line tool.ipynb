{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery command-line tool\n",
    "\n",
    "The BigQuery command-line tool is installed as part of the [Cloud SDK](https://cloud-dot-devsite.googleplex.com/sdk/docs/) and can be used to interact with BigQuery. When you use CLI commands in a notebook, the command must be prepended with a `!`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View available commands\n",
    "\n",
    "To view the available commands for the BigQuery command-line tool, use the `help` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python script for interacting with BigQuery.\n",
      "\n",
      "\n",
      "USAGE: bq.py [--global_flags] <command> [--command_flags] [args]\n",
      "\n",
      "\n",
      "Any of the following commands:\n",
      "  cancel, cp, extract, get-iam-policy, head, help, init, insert, load, ls, mk,\n",
      "  mkdef, partition, query, rm, set-iam-policy, shell, show, update, version,\n",
      "  wait\n",
      "\n",
      "\n",
      "cancel          Request a cancel and waits for the job to be cancelled.\n",
      "\n",
      "                Requests a cancel and then either: a) waits until the job is\n",
      "                done if the sync flag is set [default], or b) returns\n",
      "                immediately if the sync flag is not set. Not all job types\n",
      "                support a cancel, an error is returned if it cannot be\n",
      "                cancelled. Even for jobs that support a cancel, success is not\n",
      "                guaranteed, the job may have completed by the time the cancel\n",
      "                request is noticed, or the job may be in a stage where it cannot\n",
      "                be cancelled.\n",
      "\n",
      "                Examples:\n",
      "                bq cancel job_id  # Requests a cancel and waits until the job is\n",
      "                done.\n",
      "                bq --nosync cancel job_id  # Requests a cancel and returns\n",
      "                immediately.\n",
      "\n",
      "                Arguments:\n",
      "                job_id: Job ID to cancel.\n",
      "\n",
      "cp              Copies one table to another.\n",
      "\n",
      "                Examples:\n",
      "                bq cp dataset.old_table dataset2.new_table\n",
      "                bq cp --destination_kms_key=kms_key dataset.old_table\n",
      "                dataset2.new_table\n",
      "\n",
      "extract         Perform an extract operation of source into destination_uris.\n",
      "\n",
      "                Usage:\n",
      "                extract <source_table> <destination_uris>\n",
      "\n",
      "                Use -m option to extract a source_model.\n",
      "\n",
      "                Examples:\n",
      "                bq extract ds.table gs://mybucket/table.csv\n",
      "                bq extract -m ds.model gs://mybucket/model\n",
      "\n",
      "                Arguments:\n",
      "                source_table: Source table to extract.\n",
      "                source_model: Source model to extract.\n",
      "                destination_uris: One or more Google Cloud Storage URIs,\n",
      "                separated by\n",
      "                commas.\n",
      "\n",
      "get-iam-policy  Get the IAM policy for a resource.\n",
      "\n",
      "                Gets the IAM policy for a dataset or table resource, and prints\n",
      "                it to stdout. The policy is in JSON format.\n",
      "\n",
      "                Examples:\n",
      "                bq get-iam-policy ds\n",
      "                bq get-iam-policy proj:ds\n",
      "                bq get-iam-policy ds.table\n",
      "                bq get-iam-policy --project_id=proj -t ds.table\n",
      "\n",
      "                Note: As of September, 2019 this command is an ALPHA feature. It\n",
      "                is only enabled for customer's projects that are on the\n",
      "                feature's ALPHA list until it is released as a public generally-\n",
      "                available feature. This command may change before the public\n",
      "                release. Users who need to get access controls for BigQuery\n",
      "                resources in projects that are not enabled for this ALPHA may\n",
      "                still use the 'gcloud projects get-iam-policy' and 'bq show'\n",
      "                commands to get access controls on projects and datasets,\n",
      "                respectively.\n",
      "\n",
      "head            Displays rows in a table.\n",
      "\n",
      "                Examples:\n",
      "                bq head dataset.table\n",
      "                bq head -j job\n",
      "                bq head -n 10 dataset.table\n",
      "                bq head -s 5 -n 10 dataset.table\n",
      "\n",
      "help            Help for all or selected command:\n",
      "                bq.py help [<command>]\n",
      "\n",
      "                To retrieve help with global flags:\n",
      "                bq.py --help\n",
      "\n",
      "                To retrieve help with flags only from the main module:\n",
      "                bq.py --helpshort [<command>]\n",
      "\n",
      "init            Authenticate and create a default .bigqueryrc file.\n",
      "\n",
      "insert          Inserts rows in a table.\n",
      "\n",
      "                Inserts the records formatted as newline delimited JSON from\n",
      "                file into the specified table. If file is not specified, reads\n",
      "                from stdin. If there were any insert errors it prints the errors\n",
      "                to stdout.\n",
      "\n",
      "                Examples:\n",
      "                bq insert dataset.table /tmp/mydata.json\n",
      "                echo '{\"a\":1, \"b\":2}' | bq insert dataset.table\n",
      "\n",
      "                Template table examples: Insert to dataset.template_suffix table\n",
      "                using dataset.template table as its template.\n",
      "                bq insert -x=_suffix dataset.table /tmp/mydata.json\n",
      "\n",
      "load            Perform a load operation of source into destination_table.\n",
      "\n",
      "                Usage:\n",
      "                load <destination_table> <source> [<schema>]\n",
      "\n",
      "                The <destination_table> is the fully-qualified table name of\n",
      "                table to create, or append to if the table already exists.\n",
      "\n",
      "                The <source> argument can be a path to a single local file, or a\n",
      "                comma-separated list of URIs.\n",
      "\n",
      "                The <schema> argument should be either the name of a JSON file\n",
      "                or a text schema. This schema should be omitted if the table\n",
      "                already has one.\n",
      "\n",
      "                In the case that the schema is provided in text form, it should\n",
      "                be a comma-separated list of entries of the form name[:type],\n",
      "                where type will default to string if not specified.\n",
      "\n",
      "                In the case that <schema> is a filename, it should contain a\n",
      "                single array object, each entry of which should be an object\n",
      "                with properties 'name', 'type', and (optionally) 'mode'. See the\n",
      "                online documentation for more detail:\n",
      "                https://developers.google.com/bigquery/preparing-data-for-\n",
      "                bigquery\n",
      "\n",
      "                Note: the case of a single-entry schema with no type specified\n",
      "                is ambiguous; one can use name:string to force interpretation as\n",
      "                a text schema.\n",
      "\n",
      "                Examples:\n",
      "                bq load ds.new_tbl ./info.csv ./info_schema.json\n",
      "                bq load ds.new_tbl gs://mybucket/info.csv ./info_schema.json\n",
      "                bq load ds.small gs://mybucket/small.csv\n",
      "                name:integer,value:string\n",
      "                bq load ds.small gs://mybucket/small.csv field1,field2,field3\n",
      "\n",
      "                Arguments:\n",
      "                destination_table: Destination table name.\n",
      "                source: Name of local file to import, or a comma-separated list\n",
      "                of\n",
      "                URI paths to data to import.\n",
      "                schema: Either a text schema or JSON file, as above.\n",
      "\n",
      "ls              List the objects contained in the named collection.\n",
      "\n",
      "                List the objects in the named project or dataset. A trailing :\n",
      "                or . can be used to signify a project or dataset.\n",
      "                * With -j, show the jobs in the named project.\n",
      "                * With -p, show all projects.\n",
      "\n",
      "                Examples:\n",
      "                bq ls\n",
      "                bq ls -j proj\n",
      "                bq ls -p -n 1000\n",
      "                bq ls mydataset\n",
      "                bq ls -a\n",
      "                bq ls -m mydataset\n",
      "                bq ls --routines mydataset (requires whitelisting)\n",
      "                bq ls --filter labels.color:red\n",
      "                bq ls --filter 'labels.color:red labels.size:*'\n",
      "                bq ls --transfer_config --transfer_location='us'\n",
      "                --filter='dataSourceIds:play,adwords'\n",
      "                bq ls --transfer_run --filter='states:SUCCESSED,PENDING'\n",
      "                --run_attempt='LATEST' projects/p/locations/l/transferConfigs/c\n",
      "                bq ls --transfer_log --message_type='messageTypes:INFO,ERROR'\n",
      "                projects/p/locations/l/transferConfigs/c/runs/r\n",
      "                bq ls --capacity_commitment --project_id=proj --location='us'\n",
      "                bq ls --reservation --project_id=proj --location='us'\n",
      "                bq ls --reservation_assignment --project_id=proj --location='us'\n",
      "                bq ls --reservation_assignment --project_id=proj --location='us'\n",
      "                <reservation_id>\n",
      "                bq ls --connection --project_id=proj --location=us\n",
      "\n",
      "mk              Create a dataset, table, view, or transfer configuration with\n",
      "                this name.\n",
      "\n",
      "                See 'bq help load' for more information on specifying the\n",
      "                schema.\n",
      "\n",
      "                Examples:\n",
      "                bq mk new_dataset\n",
      "                bq mk new_dataset.new_table\n",
      "                bq --dataset_id=new_dataset mk table\n",
      "                bq mk -t new_dataset.newtable name:integer,value:string\n",
      "                bq mk --view='select 1 as num' new_dataset.newview\n",
      "                (--view_udf_resource=path/to/file.js)\n",
      "                bq mk --materialized_view='select sum(x) as sum_x from\n",
      "                dataset.table'\n",
      "                new_dataset.newview\n",
      "                bq mk -d --data_location=EU new_dataset\n",
      "                bq mk --transfer_config --target_dataset=dataset\n",
      "                --display_name=name\n",
      "                -p='{\"param\":\"value\"}' --data_source=source\n",
      "                --schedule_start_time={schedule_start_time}\n",
      "                --schedule_end_time={schedule_end_time}\n",
      "                bq mk --transfer_run --start_time={start_time}\n",
      "                --end_time={end_time}\n",
      "                projects/p/locations/l/transferConfigs/c\n",
      "                bq mk --transfer_run --run_time={run_time}\n",
      "                projects/p/locations/l/transferConfigs/c\n",
      "                bq mk --reservation --project_id=project --location=us\n",
      "                reservation_name\n",
      "                bq mk --reservation_assignment --reservation_id=project:us.dev\n",
      "                --job_type=QUERY --assignee_type=PROJECT --assignee_id=myproject\n",
      "                bq mk --reservation_assignment --reservation_id=project:us.dev\n",
      "                --job_type=QUERY --assignee_type=FOLDER --assignee_id=123\n",
      "                bq mk --reservation_assignment --reservation_id=project:us.dev\n",
      "                --job_type=QUERY --assignee_type=ORGANIZATION --assignee_id=456\n",
      "                bq mk --connection --connection_type='CLOUD_SQL'\n",
      "                --properties='{\"instanceId\" : \"instance\",\n",
      "                \"database\" : \"db\", \"type\" : \"MYSQL\" }'\n",
      "                --connection_credential='{\"username\":\"u\", \"password\":\"p\"}'\n",
      "                --project_id=proj --location=us --display_name=name\n",
      "                new_connection\n",
      "\n",
      "mkdef           Emits a definition in JSON for an external table, such as GCS.\n",
      "\n",
      "                The output of this command can be redirected to a file and used\n",
      "                for the external_table_definition flag with the \"bq query\" and\n",
      "                \"bq mk\" commands. It produces a definition with the most\n",
      "                commonly used values for options. You can modify the output to\n",
      "                override option values.\n",
      "\n",
      "                Usage:\n",
      "                mkdef <source_uris> [<schema>]\n",
      "\n",
      "                Examples:\n",
      "                bq mkdef 'gs://bucket/file.csv' field1:integer,field2:string\n",
      "\n",
      "                Arguments:\n",
      "                source_uris: a comma-separated list of uris.\n",
      "                schema: The <schema> argument should be either the name of a\n",
      "                JSON file or\n",
      "                a text schema.\n",
      "\n",
      "                In the case that the schema is provided in text form, it should\n",
      "                be a\n",
      "                comma-separated list of entries of the form name[:type], where\n",
      "                type will\n",
      "                default to string if not specified.\n",
      "\n",
      "                In the case that <schema> is a filename, it should contain a\n",
      "                single array object, each entry of which should be an object\n",
      "                with\n",
      "                properties 'name', 'type', and (optionally) 'mode'. See the\n",
      "                online\n",
      "                documentation for more detail:\n",
      "                https://developers.google.com/bigquery/preparing-data-for-\n",
      "                bigquery\n",
      "\n",
      "                Note: the case of a single-entry schema with no type specified\n",
      "                is\n",
      "                ambiguous; one can use name:string to force interpretation as a\n",
      "                text schema.\n",
      "\n",
      "partition       Copies source tables into partitioned tables.\n",
      "\n",
      "                Usage: bq partition <source_table_prefix>\n",
      "                <destination_partitioned_table>\n",
      "\n",
      "                Copies tables of the format <source_table_prefix><YYYYmmdd> to a\n",
      "                destination partitioned table, with the date suffix of the\n",
      "                source tables becoming the partition date of the destination\n",
      "                table partitions.\n",
      "\n",
      "                If the destination table does not exist, one will be created\n",
      "                with a schema and that matches the last table that matches the\n",
      "                supplied prefix.\n",
      "\n",
      "                Examples:\n",
      "                bq partition dataset1.sharded_ dataset2.partitioned_table\n",
      "\n",
      "query           Execute a query.\n",
      "\n",
      "                Query should be specifed on command line, or passed on stdin.\n",
      "\n",
      "                Examples:\n",
      "                bq query 'select count(*) from publicdata:samples.shakespeare'\n",
      "                echo 'select count(*) from publicdata:samples.shakespeare' | bq\n",
      "                query\n",
      "\n",
      "                Usage:\n",
      "                query [<sql_query>]\n",
      "\n",
      "rm              Delete the dataset, table, transfer config, or reservation\n",
      "                described by identifier.\n",
      "\n",
      "                Always requires an identifier, unlike the show and ls commands.\n",
      "                By default, also requires confirmation before deleting. Supports\n",
      "                the -d -t flags to signify that the identifier is a dataset or\n",
      "                table.\n",
      "                * With -f, don't ask for confirmation before deleting.\n",
      "                * With -r, remove all tables in the named dataset.\n",
      "\n",
      "                Examples:\n",
      "                bq rm ds.table\n",
      "                bq rm -m ds.model\n",
      "                bq rm --routine ds.routine (requires whitelisting)\n",
      "                bq rm -r -f old_dataset\n",
      "                bq rm --transfer_config=projects/p/locations/l/transferConfigs/c\n",
      "                bq rm --connection --project_id=proj --location=us con\n",
      "                bq rm --capacity_commitment proj:US.capacity_commitment_id\n",
      "                bq rm --reservation --project_id=proj --location=us\n",
      "                reservation_name\n",
      "                bq rm --reservation_assignment --project_id=proj --location=us\n",
      "                assignment_name\n",
      "\n",
      "set-iam-policy  Set the IAM policy for a resource.\n",
      "\n",
      "                Sets the IAM policy for a dataset or table resource. After\n",
      "                setting the policy, the new policy is printed to stdout.\n",
      "                Policies are in JSON format.\n",
      "\n",
      "                If the 'etag' field is present in the policy, it must match the\n",
      "                value in the current policy, which can be obtained with 'bq get-\n",
      "                iam-policy'. Otherwise this command will fail. This feature\n",
      "                allows users to prevent concurrent updates.\n",
      "\n",
      "                Usage: set-iam-policy <identifier> <filename>\n",
      "\n",
      "                The <identifier> can be an identifier for a table or dataset.\n",
      "\n",
      "                The <filename> is the name of a file containing the policy in\n",
      "                JSON format.\n",
      "\n",
      "                Examples:\n",
      "                bq set-iam-policy ds /tmp/policy.json\n",
      "                pq set-iam-policy proj:ds /tmp/policy.json\n",
      "                bq set-iam-policy ds.table /tmp/policy.json\n",
      "                bq set-iam-policy --project_id=proj -t ds.table /tmp/policy.json\n",
      "\n",
      "                Note: As of September, 2019 this command is an ALPHA feature. It\n",
      "                is only enabled for customer's projects that are on the\n",
      "                feature's ALPHA list until it is released as a public generally-\n",
      "                available feature. This command may change before the public\n",
      "                release. Users who need to set access controls for BigQuery\n",
      "                resources in projects that are not enabled for this ALPHA may\n",
      "                still use the 'gcloud projects set-iam-policy' and 'bq update'\n",
      "                commands to set access controls on projects and datasets,\n",
      "                respectively.\n",
      "\n",
      "shell           Start an interactive bq session.\n",
      "\n",
      "show            Show all information about an object.\n",
      "\n",
      "                Examples:\n",
      "                bq show -j <job_id>\n",
      "                bq show dataset\n",
      "                bq show [--schema] dataset.table\n",
      "                bq show [--view] dataset.view\n",
      "                bq show [--materialized_view] dataset.materialized_view\n",
      "                bq show -m ds.model\n",
      "                bq show --routine ds.routine (requires whitelisting)\n",
      "                bq show --transfer_config\n",
      "                projects/p/locations/l/transferConfigs/c\n",
      "                bq show --transfer_run\n",
      "                projects/p/locations/l/transferConfigs/c/runs/r\n",
      "                bq show --encryption_service_account\n",
      "                bq show --connection --project_id=project --location=us\n",
      "                connection\n",
      "                bq show --capacity_commitment project:US.capacity_commitment_id\n",
      "                bq show --reservation --location=US --project_id=project\n",
      "                reservation_name\n",
      "                bq show --reservation_assignment --project_id=project\n",
      "                --location=US\n",
      "                --assignee_type=PROJECT --assignee_id=myproject --job_type=QUERY\n",
      "                bq show --reservation_assignment --project_id=project\n",
      "                --location=US\n",
      "                --assignee_type=FOLDER --assignee_id=123 --job_type=QUERY\n",
      "                bq show --reservation_assignment --project_id=project\n",
      "                --location=US\n",
      "                --assignee_type=ORGANIZATION --assignee_id=456 --job_type=QUERY\n",
      "\n",
      "update          Updates a dataset, table, view or transfer configuration with\n",
      "                this name.\n",
      "\n",
      "                See 'bq help load' for more information on specifying the\n",
      "                schema.\n",
      "\n",
      "                Examples:\n",
      "                bq update --description \"Dataset description\" existing_dataset\n",
      "                bq update --description \"My table\"\n",
      "                existing_dataset.existing_table\n",
      "                bq update --description \"My model\" -m\n",
      "                existing_dataset.existing_model\n",
      "                bq update -t existing_dataset.existing_table\n",
      "                name:integer,value:string\n",
      "                bq update --destination_kms_key\n",
      "                projects/p/locations/l/keyRings/r/cryptoKeys/k\n",
      "                existing_dataset.existing_table\n",
      "                bq update --view='select 1 as num'\n",
      "                existing_dataset.existing_view\n",
      "                (--view_udf_resource=path/to/file.js)\n",
      "                bq update --transfer_config --display_name=name\n",
      "                -p='{\"param\":\"value\"}'\n",
      "                projects/p/locations/l/transferConfigs/c\n",
      "                bq update --transfer_config --target_dataset=dataset\n",
      "                --refresh_window_days=5 --update_credentials\n",
      "                projects/p/locations/l/transferConfigs/c\n",
      "                bq update --reservation --location=US --project_id=my-project\n",
      "                --reservation_size=2G\n",
      "                bq update --capacity_commitment --location=US --project_id=my-\n",
      "                project\n",
      "                --plan=MONTHLY --renewal_plan=FLEX\n",
      "                bq update --reservation_assignment\n",
      "                --reservation_id=proj:US.reservation1\n",
      "                reservation2.<reservation_assignment_id>\n",
      "                bq update --connection_credential='{\"username\":\"u\",\n",
      "                \"password\":\"p\"}'\n",
      "                --location=US --project_id=my-project existing_connection\n",
      "\n",
      "version         Return the version of bq.\n",
      "\n",
      "wait            Wait some number of seconds for a job to finish.\n",
      "\n",
      "                Poll job_id until either (1) the job is DONE or (2) the\n",
      "                specified number of seconds have elapsed. Waits forever if\n",
      "                unspecified. If no job_id is specified, and there is only one\n",
      "                running job, we poll that job.\n",
      "\n",
      "                Examples:\n",
      "                bq wait # Waits forever for the currently running job.\n",
      "                bq wait job_id  # Waits forever\n",
      "                bq wait job_id 100  # Waits 100 seconds\n",
      "                bq wait job_id 0  # Polls if a job is done, then returns\n",
      "                immediately.\n",
      "                # These may exit with a non-zero status code to indicate\n",
      "                \"failure\":\n",
      "                bq wait --fail_on_error job_id  # Succeeds if job succeeds.\n",
      "                bq wait --fail_on_error job_id 100  # Succeeds if job succeeds\n",
      "                in 100 sec.\n",
      "\n",
      "                Arguments:\n",
      "                job_id: Job ID to wait on.\n",
      "                secs: Number of seconds to wait (must be >= 0).\n",
      "\n",
      "\n",
      "Run 'bq.py --help' to get help for global flags.\n",
      "Run 'bq.py help <command>' to get help for <command>.\n"
     ]
    }
   ],
   "source": [
    "!bq help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a new dataset\n",
    "\n",
    "A dataset is contained within a specific [project](https://cloud.google.com/bigquery/docs/projects). Datasets are top-level containers that are used to organize and control access to your [tables](https://cloud.google.com/bigquery/docs/tables) and [views](https://cloud.google.com/bigquery/docs/views). A table or view must belong to a dataset. You need to create at least one dataset before [loading data into BigQuery](https://cloud.google.com/bigquery/loading-data-into-bigquery).\n",
    "\n",
    "First, name your new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"your_new_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates a new dataset in the US using the ID defined above.\n",
    "\n",
    "NOTE: In the examples in this notebook, the `dataset_id` variable is referenced in the commands using both `{}` and `$`. To avoid creating and using variables, replace these interpolated variables with literal values and remove the `{}` and `$` characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US mk --dataset $dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response should look like the following:\n",
    "\n",
    "```\n",
    "Dataset 'your-project-id:your_new_dataset' successfully created.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List datasets\n",
    "\n",
    "The following command lists all datasets in your default project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response should look like the following:\n",
    "\n",
    "```\n",
    "           datasetId            \n",
    " ------------------------------ \n",
    "  your_new_dataset              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from a local file to a table\n",
    "\n",
    "The following example demonstrates how to load a local CSV file into a new or existing table. See [SourceFormat](https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.job.SourceFormat.html#google.cloud.bigquery.job.SourceFormat) in the Python client library documentation for a list of available source formats. For more information, see [Loading Data into BigQuery from a local data source](https://cloud.google.com/bigquery/docs/loading-data-local) in the BigQuery documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq \\\n",
    "    --location=US \\\n",
    "    load \\\n",
    "    --autodetect \\\n",
    "    --skip_leading_rows=1 \\\n",
    "    --source_format=CSV \\\n",
    "    {dataset_id}.us_states_local_file \\\n",
    "    'resources/us-states.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Cloud Storage to a table\n",
    "\n",
    "The following example demonstrates how to load a local CSV file into a new table. See [SourceFormat](https://googleapis.github.io/google-cloud-python/latest/bigquery/generated/google.cloud.bigquery.job.SourceFormat.html#google.cloud.bigquery.job.SourceFormat) in the Python client library documentation for a list of available source formats. For more information, see [Introduction to loading data from Cloud Storage](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage) in the BigQuery documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq \\\n",
    "    --location=US \\\n",
    "    load \\\n",
    "    --autodetect \\\n",
    "    --skip_leading_rows=1 \\\n",
    "    --source_format=CSV \\\n",
    "    {dataset_id}.us_states_gcs \\\n",
    "    'gs://cloud-samples-data/bigquery/us-states/us-states.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a query\n",
    "\n",
    "The BigQuery command-line tool has a `query` command for running queries, but it is recommended to use the [magic command](./BigQuery%20Query%20Magic.ipynb) for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "The following code deletes the dataset created for this tutorial, including all tables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq rm -r -f --dataset $dataset_id"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
